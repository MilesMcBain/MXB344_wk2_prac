---
title: 'MXB344 Practical 1: San Fran Crime'
author: "Miles McBain"
date: "2 August 2016"
output:
  html_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Intro
This is a brief analysis of crime data from the summer of 2014 in the city of San Francisco. Unlike other data you may have encountered, this is public data that is not from a designed experiment. It needs a bit of work to do some analysis on. We will do this work and investigate 3 questions:

1. Can we identify the worst combinations of district, day, and time for crime? Where would you avoid?

2. Can we show visually where crime hotspots are?

3. Can we combine 1. and 2. intro a reproducible piece of analysis?

#Learning Objectives
The exercise aims to introduce you to Rmarkdown, Github, and R packages for shaping, summarising and presenting data.

#Requirements
To complete this exercise you will need a computer with R Studio installed and the following packages:

* `dplyr`
* `ggplot2`
* `readr`
* `rmarkdown`
* `leaflet`

Install with `install.packages("dplyr", "ggplot2", "readr", "rmarkdown","leaflet")`.

#Instructions

**Note** The R code in this prac may look different to what you have seen before. If so, Great! You're going to learn something. Feel free to ask for an explanation of **ANYTHING** no matter how trivial it may seem in the practical session. *R for Data Science* by Hadley Wickham is a very useful resource for the concepts covered here. See [data transformation with dplyr](http://r4ds.had.co.nz/transform.html). 

##Setting up
Assuming you are on a QUT windows PC:

1. Install missing R packages in RStudio: `install.packages("readr","leaflet","tidyr")`
2. Fork the practical repository on github.com from [here](https://github.com/MilesMcBain/MXB344_wk2_prac)
3. Install the github windows client from [https://desktop.github.com/](https://desktop.github.com/)
4. Clone your fork of the prac to a directory on your workstation's hard disk.
5. Open a local version of this source file.
6. change the `setwd()` command below to the folder where you cloned the practical repo.
7. knit this file to html.
8. Consider the discussion points in each question and if necessary, write R code to resolve them.
9. Commit your changes and sync them to your fork.
10. **Optional**. Create a pull request for the main repo from your fork.

```{r}
#setwd("~/") #Will need to set this to the right path.
```

## Load Data
Let's load the data and have a look at what we're dealing with:
```{r}
library(readr) 
sanfran_data <- read_csv("./data/sanfrancisco_incidents_summer_2014.csv")
#readr::read_csv() is a good choice over read.csv(). The main reason for this is that it never creates factors in your data frame.
#Try read.csv() if you like, to see what headaches factors can cause while you are trying to clean and preprocess data.
head(sanfran_data)
```

# Question 1
## Filtering
The question is about crime, yet we noticed from previous Load Data step there are some `NON-CRIMINAL` records mixed in. We can filter those out using `dplyr::filter`.
```{r, eval=TRUE, echo=TRUE, include=FALSE}
library(dplyr)
library(tidyr)
```
```{r}
sanfran_data <- 
  sanfran_data %>%
  filter(Category != "NON-CRIMINAL")
```
* Look at the unique entries in `sanfran_data$Category`. Are there other values you might want to filter out?


##Summarisation
We want to see if there are significant times or locations that crime peaks. To do this our data needs to be summarised according to these variables. It looks as though we already have a district vaible: `pdDistrict` and a day variable: `DayOfWeek`.

###Crimes by Day of Week and District
Below `dplyr::group_by()` and `dplyr::summarise()` work in tandem to produce the crime summary by day of week and district. `summarise()` always needs to be called on a grouped data frame.
```{r, eval=TRUE}
sanfran_data_day <- 
  sanfran_data %>%
  group_by(PdDistrict, DayOfWeek) %>%
  summarise(n_crimes = n())
head(sanfran_data_day)
```
* What kind of data is this?
* What does the function `n()` do?
* Would it be fair to model this using a linear model with a normal likelihood?
* What likelihood would you suggest?
* **Expert:** Can you create a data frame that summarises the crimes by Hour of Day, Day of Week and District?
    + Check out `dplyr::mutate()` for starters.

##Exploratory Analysis
To visualise the relationship between day, district and number of crimes a boxplot would be suitable, for example:
```{r, eval=TRUE, echo=TRUE, include=FALSE}
library(ggplot2)
```
```{r}
ggplot(data = sanfran_data_day, aes(x=PdDistrict, y=n_crimes)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90))
```

* What does the relationship for day look like?
* Try visualising the relationsip between both simultaneously using `geom_tile`. E.g. [like this](https://learnr.wordpress.com/2010/01/26/ggplot2-quick-heatmap-plotting/).

##Linear Modelling
The answer to Question 1 is probably clear by now. Depending on your audience it might be sufficent to tell the story with visuals alone. What if we need to determine if the effects are statistically significant? The only tool we have to do this at the moment are normal linear models. You should recall that fitting a pair of categorical variables to a continuous response can be done using an ANOVA. An ANOVA is really just a special name for a normal linear model with only categorical covariates. As such, it can be fit using the GLM framework using the `glm` function we have seen in class.

* Try to fit a normal linear model to this data using `glm`. But give some deep thought to evaluating the assumptions. For normal linear models with categorical covariates (ANOVA) we need/assume:
  
  + A continuous response
  + Constant variance within each day, district, and thus for the resisuals over all fitted values
  + Normal residuals 
  + Independence of observations - How do time and spatial correlation play into this issue?

* Can you justify fitting a normal linear model in this case?

#Question 2
Visualising spatial information on maps can be a powerful way to explore patterns. Historically these types of plots have been labourious to produce and required expensive tools. Luckily, there are many R packages that can facilitate this kind of plot. We will do an example using the R package `leaflet`.

## Using Coordinates
This San Fransisco Crime dataset has some coordinate variables, perfect for plotting on maps. There is the `Location` variable, which has helpfully been split out into `X` and `Y`. `X` is longitutde and `Y` is latitude. 

* If you copy some `Location` data into Google maps does it confirm of interpretation of `X` and `Y`?

##Plotting with leaflet

This uses markers with low opacity to generate a heatmap, type plot:

```{r}
library(leaflet)

sanfran_map <- 
  leaflet(data = sanfran_data) %>%
  addTiles() %>%
  addCircleMarkers(lng = sanfran_data$X, 
                   lat = sanfran_data$Y,
                   stroke = FALSE, 
                   fillOpacity = 0.02
                  )
sanfran_map

```


While this map automatically creates clusters of incidents:

```{r}
sanfran_map_cluster <- 
  leaflet(data = sanfran_data) %>%
  addTiles() %>%
  addCircleMarkers(lng = sanfran_data$X, 
                   lat = sanfran_data$Y,
                   stroke = FALSE, 
                   clusterOptions = markerClusterOptions()
                  )
sanfran_map_cluster
```


* Can you spot any outliers or concerning observations using this plot?
* How would you address this?

###Customisation
`leaflet` is an R binding of a popular Javascript library by the same name. As such, it has many options for customisation. The [documentation](https://rstudio.github.io/leaflet/basemaps.html) is clear and full of examples. A few things you can try:

* Choosing alternate map tiles
* Creating a filter control for crime categories
* Colouring crime categories
* Adding crime details to the cluster plot.

#Question 3
To complete this prac it suggested that you compile your responses into a `rmarkdown` report. Use this one as a template if you like. Or create your own structure. Ensure the whole thing can run from a new R session and generate your practical findings.

